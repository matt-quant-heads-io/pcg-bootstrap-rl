algo: PPO
total_steps: 20000 #200000
rollout_length: 128
batch_size: 256 #4096
minibatch_size: 256
update_epochs: 4
gamma: 0.99
gae_lambda: 0.95
clip_coef: 0.2
vf_coef: 0.5
ent_coef: 0.01
learning_rate: 0.0003
eval_episodes: 10
checkpoint_interval: 5000
